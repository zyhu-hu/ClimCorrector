# A Guide for Deep Learning  

Author: Zeyuan Hu
02/19/2025

## Table of Contents
- [1. Learning basic deep learning concepts](#1-learning-basic-deep-learning-concepts)
    - [Coursera](#coursera)
    - [More advanced architectures](#more-advanced-architectures)
    - [Diffusion Model](#diffusion-model)
- [2. Object-Oriented Programming (OOP）](#2-object-oriented-programming-oop)
    - [2.1 Classes and Instances](#21-classes-and-instances)
    - [2.2 Inheritance](#22-inheritance)
    - [2.3 Methods and Method Overriding](#23-methods-and-method-overriding)
    - [2.4 Encapsulation](#24-encapsulation)
    - [2.5 Composition](#25-composition)
    - [2.6 More resources](#26-more-resources)
- [3. PyTorch](#3-pytorch)
    - [3.1 Official PyTorch Documentation](#31-official-pytorch-documentation)
    - [3.2 Some more advanced topics](#32-some-more-advanced-topics)
        - [3.2.1 Distributed training](#321-distributed-training)
        - [3.2.2 Other random topics](#322-other-random-topics)


## 1. Learning basic deep learning concepts

### Coursera
 
I took the coursera's [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) by Andrew Ng. This course is not entirely free but it has a 7-day free trial. I don't recommend doing its assignments because it uses Tensorflow. PyTorch is more popular now from my personal perspective, and I highly recommend you to start with pytorch instead of tensorflow. I recommend using this course to learn the basic concepts. I believe there are also other similar online courses from different universities's youtube channels as well as on udemy/fast.ai.

Some key concepts that you may need to understand are: forward and backward propagation, train/validation/test split, regularization, dropout, normalization, vanishing/exploding gradients, weight initialization, batch, epoch, loss function, gradient descent, optimizers, learning rate, learning rate scheduler, hyperparameter tuning, batch/layer normalization, transfer learning, basics of convolutional neural networks (CNN), application of CNN on different computer vision tasks such as classification and segmentation, sequence models like recurrent neural networks, attention mechanism, transformers.

### More advanced architectures

I found this [EfficientML](https://hanlab.mit.edu/courses/2024-fall-65940) course from MIT to be very helpful. It covers more advanced topics like efficient training, model compression, and state-of-the-art architectures/models. Several popular architectures used in current AI for climate/weather research include but not limited to U-Net, transformer, vision transformer, variational autoencoder (VAE). Lecture 12 from this course could be a very good introduction to transformers. Lecture 16 could be a good introduction to vision transformers. There are also many short youtube videos that explain these concepts in a simple way. It would be good to know the time complexity of a naive vision transformer and what are some variants of vision transformers that can reduce the time complexity, like SwinTransformer, AFNO, etc which are used in many recent weather applications.

Usually you don't need to write a complex architecture from the scratch. You can find many implementations from other researchers on github. You can start with their code and modify it to fit your needs. However, it could be useful to learn how a basic transformer or vision transformer is implemented (after you've got a basic understanding of how PyTorch works).

### Diffusion Model

This is a more advanced topic. In case you are interested in it, here are some very useful resources:
[a tutorial from CVPR 2022](https://www.youtube.com/watch?v=cS6JQpEY9cs&t=8158s).
[a presentation recording of Yang Song](https://www.youtube.com/watch?v=cS6JQpEY9cs&t=8158s), who is the leading author of the score-based generative model. I found he is very good at explaining the concepts in a clear way.
[Elucidating the Design Space of Diffusion-Based Generative Models](https://www.youtube.com/watch?v=T0Qxzf0eaio&t=2672s) is a very good paper to optimize the design of a diffusion model. Many recent diffusion-based weather applications, like GenCase from Deepmind or Corrdiff from Nvidia, are based on the design of this paper.
[EfficientML](https://hanlab.mit.edu/courses/2024-fall-65940)'s lecture 18 is also a good introduction to diffusion models.

## 2. Object-Oriented Programming (OOP）

While not 100% necessary, understanding the basics of OOP can help you understand PyTorch's design and structure. Here are some key concepts to focus on:
Classes and instances, inheritance, methods and method overriding, encapsulation, composition. Below are more detailed examples (generated by gpt but illustrative).

## 2.1 Classes and Instances  
```python
class MyModel:
    def __init__(self, name):
        self.name = name  # Instance attribute

    def print_name(self):
        print(f"Model name: {self.name}")

model = MyModel("ConvNet")
model.print_name()  # Output: Model name: ConvNet
```
In PyTorch, models are instances of torch.nn.Module, so you need to understand how to create and use objects.

## 2.2 Inheritance  
```python
import torch.nn as nn

class MyNetwork(nn.Module):  # Inherit from nn.Module
    def __init__(self, input_size, output_size):
        super(MyNetwork, self).__init__()  # Call parent class constructor
        self.fc = nn.Linear(input_size, output_size)

    def forward(self, x):  # Define how the model processes input
        return self.fc(x)

# Create an instance of the model
model = MyNetwork(10, 2)
print(model)
```
PyTorch models are subclasses of `torch.nn.Module`. PyTorch’s `nn.Module` provides useful built-in functionality (like `.parameters()` and `.to(device)`). Using `super().__init__()` ensures the parent class (nn.Module) initializes correctly.

## 2.3 Methods and Method Overriding  
```python
class SimpleNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):  # Defines forward pass
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x
```
Every PyTorch model needs a forward method to define computation. This method is called when you pass input data to the model, like `output = model(input)`.

## 2.4 Encapsulation  
```python
class ModelWithDropout(nn.Module):
    def __init__(self):
        super(ModelWithDropout, self).__init__()
        self.linear = nn.Linear(10, 10)
        self.dropout = nn.Dropout(p=0.5)  # Dropout layer

    def forward(self, x):
        x = self.linear(x)
        x = self.dropout(x)  # Dropout only active during training
        return x
```
Encapsulation means bundling related variables and functions inside a class. PyTorch models usually store layers as instance attributes (self.linear, self.dropout). Encapsulation keeps code organized (you don’t define layers in forward() directly).

## 2.5 Composition  
```python
class Block(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(Block, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        self.activation = nn.ReLU()

    def forward(self, x):
        return self.activation(self.linear(x))

class DeepNetwork(nn.Module):
    def __init__(self):
        super(DeepNetwork, self).__init__()
        self.block1 = Block(10, 20)  # Using Block inside the model
        self.block2 = Block(20, 10)

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        return x
```
Instead of defining everything in one class, you can reuse smaller modules inside a bigger model. In this example, we define a Block class and use it inside DeepNetwork. This is called composition.

## 2.6 More resources
I learned OOP through Harvard's [AC207](https://harvard-iacs.github.io/2020-CS107/). Check the slides from lecture 5-8, which should probably be enough.


## 3. PyTorch

### 3.1 Official PyTorch Documentation
First it would be good to learn the basics from the official [PyTorch documentation](https://pytorch.org/tutorials/). This official [quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) tutorial is a good starting point that already covers the most basic concepts for training a NN with PyTorch. These basic concepts could include: DataLoader, Dataset, Define a model, Define a loss function, Define an optimizer, Train the model, Evaluate the model, Saving and loading the model.

### 3.2 Some more advanced topics

After you've got a basic understanding of PyTorch, it may be already fine to work with my code. Check the other tutorial in this repo on how to reproduce my training process for a NN-based bias corrector. Below are some topics that you may want to know in advance to understand my code better.

#### 3.2.1 Distributed training

Usually you want to use more than one gpus to speed up the training process. There are several types of distributed training, like Data Parallel, Pipeline Parallel, Tensor Parallel. The most common/basic one is Data Parallel. You can find a good introduction from [EfficientML](https://hanlab.mit.edu/courses/2024-fall-65940) course's lecture 19 to understand the concept. It would be good to understand how data parallel works under the hood, i.e., how gradients and model weights are handled on multiple GPUs. The official [PyTorch documentation](https://pytorch.org/tutorials/distributed/home.html) also provides a good tutorial on how to use DistributedDataParallel (DDP) and others to train a model on multiple GPUs.

#### 3.2.2 Other random topics

- Learning rate scheduler: a good learning rate scheduler can help you train a model faster and more efficiently. Learning rate scheduler manages how the learning rate changes over training steps or epochs. I usually use `optim.lr_scheduler.ReduceLROnPlateau`, `optim.lr_scheduler.StepLR`, or `optim.lr_scheduler.CosineAnnealingLR`. 
- Dataloader best practices: Sometimes the data loading process could be the bottleneck of the training process. It would be good to learn how to optimize the data loading process. For example, you can set `num_workers` to load data in parallel using multiple cpus. If the entire training data is too large to fit into your GPU memory at once, you can define the Dataloader to only the piece of data when needed (e.g., using hdf5 format).
- TorchScript: This is a technique to convert a PyTorch model to a serialized format that can be run in a non-Python environment. This is useful when you want to deploy the model in a Fortran-based code.
- [wandb](https://docs.wandb.ai/quickstart/): This is a tool to log your training process. It is very useful to track the training process and compare different experiments. I highly recommend using it.
- [hydra](https://hydra.cc/docs/intro/): This is a tool to manage your configuration files. It is very useful when you have many hyperparameters to tune. It allows you to easily change the hyperparameters in your slurm job script without changing the python code.
- modulus: This is a Nvidia-developed library to optimize the training process. It includes out-of-box features like CUDA Graph, AMP (Automatic Mixed Precision), JIT (Just-In-Time Compilation) to speed up the training process. It only requires small changes from usual PyTorch code. Check this [quick tutorial](https://docs.nvidia.com/deeplearning/modulus/modulus-core/tutorials/simple_training_example.html) to know the basics of how to use modulus.

